# -*- coding: utf-8 -*-
"""Pubmed_Inference_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kdi4ShgVW3_2Enuium3kvq4_iQZz7LYN

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).

[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!

Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.

Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
#     xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# !pip install transformers==4.56.2
# !pip install --no-deps trl==0.22.2

"""### Unsloth

If you want to finetune Llama-3 2x faster and use 70% less VRAM, go to our [finetuning notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)!
"""

from unsloth import FastLanguageModel

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/gemma-7b-it-bnb-4bit",
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",

] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct",
    max_seq_length = 8192,
    load_in_4bit = True,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

from transformers import TextStreamer
from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

"""Change the "value" part to call the model!

Unsloth makes inference natively 2x faster!! No need to change or do anything!

#Just predict yes no maybe
"""

# PubMedQA — single-token label + Brier from logits (Unsloth one-by-one)

import re, gc, torch
from datasets import load_dataset
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report
# Adapter to use Unsloth / HF models with the reflexion agents
from hotpotqa_runs.hf_llm import AnyHFLLM
from hotpotqa_runs.agents import CoTAgent, ReflexionStrategy
from hotpotqa_runs.prompts import pubmed_agent_prompt, pubmed_reflect_prompt

DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
MAX_INP = 768      # lower -> faster prefill
SYSTEM  = "Answer ONLY with one word: yes, no, or maybe."
CLASSES = ["yes","no","maybe"]
CLASS_SET = set(CLASSES)

# ---------- prompt: make the first generated token be the label ----------
def build_messages(q, ctx):
    if isinstance(ctx, list): ctx = " ".join(ctx)
    return [{
        "from": "human",
        "value": (
            f"{SYSTEM}\n\nQuestion: {q}\n\nAbstract:\n{ctx}\n\n"
            "Answer strictly with one word.\n\n"
            "Label: "   # <- next token will be yes|no|maybe
        )
    }]

def prompt_text(row):
    return tokenizer.apply_chat_template(
        build_messages(row["question"], row["context"]),
        tokenize=False, add_generation_prompt=True
    )

# ---------- tiny helpers ----------
def parse_label_token(tok_text: str) -> str:
    lab = tok_text.strip().strip(",.?:;!").lower()
    return lab if lab in CLASS_SET else "maybe"

def _first_token_ids(strings):
    out = []
    for s in strings:
        ids = tokenizer(s, add_special_tokens=False).input_ids
        if ids: out.append(ids[0])
    return out

# include space/no-space + case variants (Llama tokenizers often use leading-space tokens)
CAND_IDS = {
    "yes":   _first_token_ids([" yes","Yes","yes"]),
    "no":    _first_token_ids([" no","No","no"]),
    "maybe": _first_token_ids([" maybe","Maybe","maybe"]),
}

def probs_from_first_step_logits(out_struct):
    logits = out_struct.scores[0][0]    # (vocab,)
    pv = torch.softmax(logits, dim=-1)
    mass = {
        lab: float(pv[torch.tensor(ids, device=pv.device)].sum().item()) if ids else 0.0
        for lab, ids in CAND_IDS.items()
    }
    Z = sum(mass.values()) + 1e-12
    return {k: v/Z for k, v in mass.items()}

def brier_multiclass_sum(prob_dict, gold_label, classes=CLASSES):
    # Sum version ranges [0, 2] for 3 classes (0 is perfect)
    return sum((prob_dict[c] - (1.0 if c == gold_label else 0.0))**2 for c in classes)

# ---------- data ----------
ds   = load_dataset("qiaojin/PubMedQA", "pqa_labeled")
pmqa = ds["train"]       # use "test" for reporting; change to "train" if you want
N    = len(pmqa)        # set smaller for a smoke test

preds, golds = [], []
brier_probs, brier_vals = [], []

for i in tqdm(range(N), desc="Label-only + Brier (one-by-one)", ncols=100):
    row  = pmqa[i]
    gold = row["final_decision"].lower()
    golds.append(gold)

    prompt = prompt_text(row)
    enc = tokenizer(prompt, return_tensors="pt",
                    padding=False, truncation=True, max_length=MAX_INP).to(DEVICE)

    with torch.inference_mode():
        out = model.generate(
            **enc,
            max_new_tokens=1,            # exactly the label token
            do_sample=False, temperature=0.0,
            use_cache=False,             # lower KV mem
            pad_token_id=tokenizer.eos_token_id,
            output_scores=True,          # <- logits for first step
            return_dict_in_generate=True
        )

    # decode ONLY the new token
    new_tok = out.sequences[0, enc.input_ids.shape[1]:]
    label_text = tokenizer.decode(new_tok, skip_special_tokens=True)
    pred = parse_label_token(label_text)
    preds.append(pred)

    # probs -> Brier
    probs = probs_from_first_step_logits(out)
    brier_probs.append(probs)
    brier_vals.append(brier_multiclass_sum(probs, gold))

    del enc, out, new_tok
    if (i+1) % 100 == 0 and torch.cuda.is_available():
        torch.cuda.empty_cache(); gc.collect()

# ---------- metrics ----------
print(f"\nAccuracy:  {accuracy_score(golds, preds):.4f}")
print(f"Macro-F1:  {f1_score(golds, preds, average='macro'):.4f}\n")
print(classification_report(golds, preds, digits=4))

print(f"\nMean Brier (sum, 0–2): {sum(brier_vals)/len(brier_vals):.6f}")

# peek a few
for j in range(min(5, N)):
    print(f"[{j:03d}] gold={golds[j]:<6} pred={preds[j]:<6}  probs={brier_probs[j]}")

"""# predict + compute readability, brier score and  rogue1"""

pip install rouge_score

import re, gc, torch
from datasets import load_dataset
from tqdm.auto import tqdm
from statistics import mean

# Optional deps (install once in Colab if missing)
try:
    from evaluate import load as load_metric
except Exception:
    !pip -q install evaluate
    from evaluate import load as load_metric

try:
    import textstat
except Exception:
    !pip -q install textstat
    import textstat

DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"

# ==== PubMedQA rationale-only eval (ROUGE-1 + readability) ====
# Assumes:
#   - model, tokenizer already loaded (Unsloth)
#   - tokenizer has chat template set (e.g., get_chat_template(..., "llama-3.1"))
#   - FastLanguageModel.for_inference(model) already called



# ---- knobs (no abstract shrinking) ----
MAX_INP = 1024    # cap for (prompt + abstract) tokens
MAX_NEW = 160     # generation budget for rationale

# ---- Single-pass prompt: ONLY rationale required ----
INSTR = (
    "You are answering PubMedQA. "
    "Write a concise explanation in plain language based only on the abstract. "
    "End with: 'This is not medical advice.'\n\n"
    "Return answers in this EXACT format:\n"
    "Reason:\n"
    "<your explanation>"
)

def build_messages(q, ctx):
    if isinstance(ctx, list): ctx = " ".join(ctx)
    return [{
        "from": "human",
        "value": f"{INSTR}\n\nQuestion: {q}\n\nAbstract:\n{ctx}\n\nReason:\n"
    }]

def apply_tpl(msgs):
    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

# ---- helpers to clean and extract the rationale ----
ASSIST_RE = re.compile(
    r'^(?:<\|assistant\|>|<\|start_header_id\|>\s*assistant\s*<\|end_header_id\|>|assistant:?)[\s\r\n]*',
    re.IGNORECASE
)
def strip_assistant_header(text: str) -> str:
    text = text.lstrip()
    text = ASSIST_RE.sub("", text)
    lines = [ln.strip() for ln in text.splitlines()]
    while lines and re.fullmatch(r'(?:assistant:?|<\|assistant\|>)', lines[0], re.IGNORECASE):
        lines.pop(0)
    return "\n".join(lines).strip()

def parse_reason(out_text: str) -> str:
    t = strip_assistant_header(out_text)
    m = re.search(r'(?mi)^Reason:\s*(.*)$', t, flags=re.DOTALL)
    if m:
        return m.group(1).strip()
    # fallback: everything after the last "Reason:"
    cut = t.lower().rfind("reason:")
    return t[cut+len("reason:"):].strip() if cut != -1 else t

# ---- load data ----
ds   = load_dataset("qiaojin/PubMedQA", "pqa_labeled")
pmqa = ds["train"]      # change to "test" if you want test-set numbers
N    = len(pmqa)        # set smaller for a smoke test, e.g., N = 100

# ---- prebuild prompts (saves a little time) ----
prompts = [apply_tpl(build_messages(ex["question"], ex["context"])) for ex in pmqa]

# ---- generation loop ----
refs_long, hyps_long = [], []
for i in tqdm(range(N), desc="Generating rationales", ncols=100):
    ex = pmqa[i]
    refs_long.append((ex.get("long_answer") or "").strip())

    p = prompts[i]
    enc = tokenizer(
        p, return_tensors="pt",
        padding=False, truncation=True, max_length=MAX_INP
    ).to(DEVICE)

    with torch.inference_mode():
        out = model.generate(
            **enc,
            max_new_tokens=MAX_NEW,
            do_sample=False, temperature=0.0,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            return_dict_in_generate=True,
        )

    # decode only new tokens
    new_tokens = out.sequences[0, enc.input_ids.shape[1]:]
    gen_text   = tokenizer.decode(new_tokens, skip_special_tokens=True)
    rationale  = parse_reason(gen_text)
    hyps_long.append(rationale)

    # free per-iteration
    del enc, out, new_tokens
    if (i+1) % 100 == 0 and torch.cuda.is_available():
        torch.cuda.empty_cache(); gc.collect()

# ---- evaluation: ROUGE-1 + readability ----
rouge = load_metric("rouge")
r = rouge.compute(predictions=hyps_long, references=refs_long, use_stemmer=True)
print(f"\nROUGE-1 (rationales vs gold long_answer), n={len(hyps_long)}: {float(r['rouge1']):.6f}")

fre  = [textstat.flesch_reading_ease(h) for h in hyps_long]
fk   = [textstat.flesch_kincaid_grade(h) for h in hyps_long]
smog = [textstat.smog_index(h) for h in hyps_long]
print("\nReadability of generated rationales (mean):")
print(f"  Flesch Reading Ease:   {mean(fre):.2f}")
print(f"  Flesch-Kincaid Grade:  {mean(fk):.2f}")
print(f"  SMOG Index:            {mean(smog):.2f}")

# ---- show first 3 model answers for sanity ----
for j in range(min(3, N)):
    ex  = pmqa[j]
    q   = ex["question"]
    ref = refs_long[j]
    hyp = hyps_long[j]
    print(f"\n[{j:03d}]")
    print("Q:", q)
    print("Gold (first 220ch):", (ref[:220] + "…") if len(ref) > 220 else ref)
    print("Hyp  (first 220ch):", (hyp[:220] + "…") if len(hyp) > 220 else hyp)

# -------------------------
# Demo: run reflexion agents (CoTAgent) with Unsloth adapter
# -------------------------
# Create HF adapter (re-uses `model` and `tokenizer` loaded above)
hf_llm = AnyHFLLM(model=model, tokenizer=tokenizer, temperature=0.0, max_tokens=128, device=DEVICE)

# Run CoTAgent on a few examples and allow one reflexion retry
print('\n=== Demo: CoTAgent with Reflexion (Unsloth HF adapter) ===')
for j in range(min(5, N)):
    ex = pmqa[j]
    question = ex['question']
    context = ex.get('context', '')
    gold = ex.get('final_decision', '')

    agent = CoTAgent(
        question=question,
        context=context,
        key=gold,
        self_reflect_llm=hf_llm,
        action_llm=hf_llm,
        agent_prompt=pubmed_agent_prompt,
        reflect_prompt=pubmed_reflect_prompt,
    )

    # First attempt
    agent.run(reflexion_strategy=ReflexionStrategy.NONE)

    # If incorrect, run reflexion then retry
    if not agent.is_correct():
        agent.run(reflexion_strategy=ReflexionStrategy.REFLEXION)

    print(f"[demo {j}] gold={gold} pred={agent.answer}\n")

ref

"""And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️

  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)
</div>

"""