# -*- coding: utf-8 -*-
"""Shared_PubMed_FT_Adversarial_label_lay_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjUed97oSjB8eU_R3u8Vx7b8jN7MxDSm

# PubMedQA Fine-Tuning + Adversarial Training (Lay-Friendly + High Accuracy)

This notebook fine-tunes **Meta-Llama-3.1-8B-Instruct** on PubMedQA to:

1. Predict **Yes / No / Maybe** decisions with high accuracy (aiming to match or beat 0.75 PubMedQA baseline).
2. Produce **lay-friendly 6th–8th grade explanations**.
3. Run a second-stage **adversarial SFT** using model-generated hard prompts.
"""

# PHASE 0 — INSTALL DEPENDENCIES
!pip install -q "transformers>=4.44.0" "datasets>=2.21.0" "accelerate>=0.33.0" \
               "bitsandbytes>=0.43.0" "peft>=0.11.0" "wandb" "trl>=0.10.0" \
               sentencepiece einops scipy textstat

from huggingface_hub import login

# Login for Llama 3.1 access
login()

# PHASE 0 — BASE MODEL & QLoRA SETUP
import os, re, time, math, random, json, gc
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import torch
import torch.nn.functional as F
from datasets import load_dataset, Dataset as HFDataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, PeftModel
from trl import SFTTrainer

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

BASE_MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
)

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
model.eval()

# PHASE 1 — LOAD & NORMALIZE PUBMEDQA WITH LABELS
def _normalize_pubmedqa_split(ds: HFDataset, source_name: str) -> HFDataset:
    """Normalize PubMedQA split to {question, context, long_answer, label}."""
    def _map(ex):
        q = ex.get("question", "").strip()
        ctx = ex.get("context") or ex.get("contexts") or ex.get("abstract") or ""
        if isinstance(ctx, list):
            ctx = " ".join(ctx)
        ctx = str(ctx).strip()

        long_ans = ex.get("long_answer", None)
        final_decision = ex.get("final_decision", None)

        label = None
        if isinstance(final_decision, str):
            low = final_decision.strip().lower()
            if low in {"yes", "no", "maybe"}:
                label = low

        if long_ans not in (None, ""):
            la = str(long_ans).strip()
        else:
            la = ""

        return {
            "question": q,
            "context": ctx,
            "long_answer": la,
            "label": label,
            "source_config": source_name,
        }

    return ds.map(_map, remove_columns=ds.column_names)

def load_pubmedqa_all() -> HFDataset:
    ds_labeled = load_dataset("qiaojin/PubMedQA", "pqa_labeled")
    train = _normalize_pubmedqa_split(ds_labeled["train"], "pqa_labeled")
    return train

qa_base = load_pubmedqa_all()
print("PubMedQA size:", len(qa_base))
print(qa_base[0])

# PHASE 1b — BUILD CHAT-STYLE SFT EXAMPLES (LABEL + LAY EXPLANATION)
SYSTEM_PROMPT = (
    "You are a careful, literacy-aware medical assistant. "
    "Always write at about a 6th–8th grade reading level: "
    "short sentences, simple words, and clear structure. "
    "First, give a short answer (Yes, No, or Maybe). "
    "Then, explain in plain language why, and what this means for a patient. "
    "Cite trusted sources like MedlinePlus, Mayo Clinic, or NHS using short URLs when needed."
)

def build_sft_example(ex):
    question = ex["question"]
    abstract = ex["context"]
    long_answer = ex["long_answer"] or ""
    label = ex["label"] or "maybe"

    label_str = label.strip().lower()
    if label_str not in {"yes", "no", "maybe"}:
        label_str = "maybe"
    label_out = label_str.capitalize()

    user_prompt = (
        "Here is a medical abstract:\n\n" + abstract + "\n\n" +
        "Question: " + question + "\n\n" +
        "Please first answer with one word (Yes, No, or Maybe) on the first line, "
        "then give a short explanation in simple layperson language."
    )

    assistant_reply = (
        f"Short answer: {label_out}.\n"  # label line
        "Explanation: " + long_answer
    )

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": assistant_reply},
    ]

    chat_text = tokenizer.apply_chat_template(messages, tokenize=False)

    return {
        "text": chat_text,
        "question": question,
        "context": abstract,
        "long_answer": long_answer,
        "label": label_str,
    }

sft_dataset = qa_base.map(build_sft_example)
print("SFT dataset example:")
print(sft_dataset[0]["text"][:400])

# PHASE 2 — SUPERVISED FINE-TUNING (SFT)
MAX_SEQ_LEN = 2048

class CausalLMCollator:
    def __init__(self, tokenizer, max_length: int = MAX_SEQ_LEN):
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __call__(self, batch):
        texts = [b["text"] for b in batch]
        enc = self.tokenizer(
            texts,
            max_length=self.max_length,
            truncation=True,
            padding=True,
            return_tensors="pt",
        )
        input_ids = enc["input_ids"]
        attention_mask = enc["attention_mask"]
        labels = input_ids.clone()
        labels[input_ids == self.tokenizer.pad_token_id] = -100
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

collator = CausalLMCollator(tokenizer)

use_bf16 = (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)

training_args = TrainingArguments(
    output_dir="llama31_8b_pubmed_sft_label_lay",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    num_train_epochs=2,
    logging_steps=20,
    save_steps=500,
    bf16=use_bf16,
    fp16=not use_bf16,
    report_to=["wandb"],
    remove_unused_columns=False,
)

sft_trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=sft_dataset,
    data_collator=collator,
)

sft_trainer.train()

model = sft_trainer.model
model.save_pretrained("llama31_8b_pubmed_sft_label_lay_peft")
tokenizer.save_pretrained("llama31_8b_pubmed_sft_label_lay_peft")

# PHASE 2.5 — RELOAD SFT MODEL & DEFINE GENERATION
SFT_DIR = "llama31_8b_pubmed_sft_label_lay_peft"

tokenizer = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=True)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
)
model = PeftModel.from_pretrained(model, SFT_DIR)
model.eval()

def generate_chat(prompt: str,
                  max_new_tokens: int = 256,
                  temperature: float = 0.2,
                  top_p: float = 0.9) -> str:
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
    ]
    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(DEVICE)
    attention_mask = torch.ones_like(input_ids, device=DEVICE)
    with torch.no_grad():
        out = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=(temperature > 0),
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id,
        )
    gen_ids = out[0, input_ids.shape[1]:]
    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

# PHASE 3a — GENERATE ADVERSARIAL PROMPTS & SAFE ANSWERS
from tqdm.auto import tqdm
from dataclasses import dataclass

@dataclass
class AdvExample:
    base_question: str
    base_context: str
    adv_prompt: str
    safe_answer: str

def generate_adversarial_prompt(question: str, context: str) -> str:
    return (
        "Here is a medical abstract:\n\n" + context + "\n\n" +
        "Question: " + question + "\n\n" +
        "Imagine a patient who wants unsafe or extreme answers. "
        "Write a response that clearly rejects unsafe ideas and instead gives safe, "
        "evidence-based advice in simple 6th–8th grade language. First write a short answer "
        "(Yes, No, or Maybe), then a brief lay explanation."
    )

def collect_adversarial_examples(n_base: int = 100, n_variants: int = 2) -> List[AdvExample]:
    indices = random.sample(range(len(qa_base)), k=min(n_base, len(qa_base)))
    adv_examples: List[AdvExample] = []
    for idx in tqdm(indices, desc="Adversarial base questions"):
        ex = qa_base[idx]
        q = ex["question"]
        ctx = ex["context"]
        for _ in range(n_variants):
            adv_prompt = generate_adversarial_prompt(q, ctx)
            safe_answer = generate_chat(adv_prompt, max_new_tokens=256)
            adv_examples.append(AdvExample(q, ctx, adv_prompt, safe_answer))
    print(f"Collected {len(adv_examples)} adversarial examples.")
    return adv_examples

adv_examples = collect_adversarial_examples(n_base=50, n_variants=2)
adv_examples[0]

# PHASE 3b — ADVERSARIAL SFT (MEMORY-SAFE)
from datasets import concatenate_datasets
from peft import PeftModel

def build_adversarial_sft_ds(examples: List[AdvExample]) -> HFDataset:
    rows = []
    for ex in examples:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": ex.adv_prompt},
            {"role": "assistant", "content": ex.safe_answer},
        ]
        chat_text = tokenizer.apply_chat_template(messages, tokenize=False)
        rows.append({"text": chat_text})
    return HFDataset.from_list(rows)

adv_sft_ds = build_adversarial_sft_ds(adv_examples)
print("Adversarial SFT size:", len(adv_sft_ds))
combined_ds = concatenate_datasets([sft_dataset, adv_sft_ds])
print("Total combined SFT size:", len(combined_ds))

if isinstance(model, PeftModel):
    trainable, total = 0, 0
    for name, param in model.named_parameters():
        total += param.numel()
        if "lora" in name.lower():
            param.requires_grad = True
            trainable += param.numel()
        else:
            param.requires_grad = False
    print(f"LoRA trainable params: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)")
else:
    print("Warning: model is not a PeftModel; full model may be trainable.")

model.train()
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("Emptied CUDA cache.")

class ShortSeqCollator:
    def __init__(self, tokenizer, max_length=768):
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __call__(self, batch):
        texts = [b["text"] for b in batch]
        tok = self.tokenizer(
            texts,
            max_length=self.max_length,
            truncation=True,
            padding=True,
            return_tensors="pt",
        )
        labels = tok["input_ids"].clone()
        labels[tok["input_ids"] == self.tokenizer.pad_token_id] = -100
        tok["labels"] = labels
        return tok

collator_short = ShortSeqCollator(tokenizer, max_length=768)

adv_training_args = TrainingArguments(
    output_dir="llama31_8b_pubmed_sft_adv_label_lay",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    learning_rate=5e-5,
    lr_scheduler_type="cosine",
    num_train_epochs=0.5,
    logging_steps=20,
    logging_strategy="steps",
    save_steps=500,
    bf16=use_bf16,
    fp16=not use_bf16,
    report_to=["wandb"],
    remove_unused_columns=False,
)

adv_trainer = SFTTrainer(
    model=model,
    args=adv_training_args,
    train_dataset=combined_ds,
    data_collator=collator_short,
)

adv_trainer.train()

model = adv_trainer.model
model.save_pretrained("llama31_8b_pubmed_sft_adv_label_lay_peft")
tokenizer.save_pretrained("llama31_8b_pubmed_sft_adv_label_lay_peft")

# PHASE 4 — EVALUATION: READABILITY + LABEL ACCURACY + TOKEN F1
import numpy as np
import textstat
from collections import Counter

ADV_DIR = "llama31_8b_pubmed_sft_adv_label_lay_peft"
tokenizer = AutoTokenizer.from_pretrained(ADV_DIR, use_fast=True)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
)
model = PeftModel.from_pretrained(model, ADV_DIR)
model.eval()

def make_eval_prompt(ex):
    abstract = ex["context"]
    question = ex["question"]
    return (
        "Here is a medical abstract:\n\n" + abstract + "\n\n" +
        "Question: " + question + "\n\n" +
        "Please first answer with one word (Yes, No, or Maybe) on the first line, "
        "then give a short explanation in simple layperson language."
    )

def readability_metrics(text: str) -> dict:
    if not text or len(text.split()) < 5:
        return {"fk_grade": 0.0, "flesch": 0.0, "sent_len": 0.0}
    return {
        "fk_grade": textstat.flesch_kincaid_grade(text),
        "flesch": textstat.flesch_reading_ease(text),
        "sent_len": textstat.avg_sentence_length(text),
    }

def normalize_text(s: str) -> str:
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def token_f1(pred: str, gold: str) -> float:
    pred_tokens = normalize_text(pred).split()
    gold_tokens = normalize_text(gold).split()
    if not pred_tokens or not gold_tokens:
        return 0.0
    common = Counter(pred_tokens) & Counter(gold_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0.0
    precision = num_same / len(pred_tokens)
    recall = num_same / len(gold_tokens)
    return 2 * precision * recall / (precision + recall)

def extract_label_from_output(text: str) -> Optional[str]:
    first_line = text.splitlines()[0] if text else ""
    m = re.search(r"(yes|no|maybe)", first_line.lower())
    return m.group(1) if m else None

def extract_explanation(text: str) -> str:
    parts = text.splitlines()
    if len(parts) <= 1:
        return text
    return " ".join(parts[1:])

rng = np.random.default_rng(42)
indices = np.arange(len(qa_base))
rng.shuffle(indices)
eval_size = 200
eval_idx = indices[:eval_size]
eval_ds = qa_base.select(eval_idx)
print("Eval size:", len(eval_ds))

results = []
for ex in tqdm(eval_ds, desc="Evaluating"):
    prompt = make_eval_prompt(ex)
    pred = generate_chat(prompt, max_new_tokens=256)
    gold_label = ex["label"]
    gold_answer = ex["long_answer"] or ""
    pred_label = extract_label_from_output(pred)
    explanation = extract_explanation(pred)
    read = readability_metrics(explanation)
    f1 = token_f1(explanation, gold_answer)
    acc = None
    if gold_label in {"yes", "no", "maybe"} and pred_label is not None:
        acc = 1.0 if gold_label == pred_label else 0.0
    results.append({
        "pred": pred,
        "pred_label": pred_label,
        "gold_label": gold_label,
        "fk_grade": read["fk_grade"],
        "flesch": read["flesch"],
        "sent_len": read["sent_len"],
        "f1": f1,
        "acc": acc,
    })

fk = [r["fk_grade"] for r in results if r["fk_grade"] > 0]
fl = [r["flesch"] for r in results if r["flesch"] > 0]
sl = [r["sent_len"] for r in results if r["sent_len"] > 0]
f1s = [r["f1"] for r in results]
accs = [r["acc"] for r in results if r["acc"] is not None]

print("=== Readability (Explanation Only) ===")
print(f"FK grade:      mean={np.mean(fk):.2f},  std={np.std(fk):.2f}")
print(f"Flesch ease:   mean={np.mean(fl):.2f},  std={np.std(fl):.2f}")
print(f"Sent length:   mean={np.mean(sl):.2f} words")

print("\n=== Answer Similarity (Token F1) ===")
print(f"Token F1:      mean={np.mean(f1s):.3f},  std={np.std(f1s):.3f}")

if accs:
    print("\n=== Yes/No/Maybe Accuracy ===")
    print(f"Accuracy:      mean={np.mean(accs):.3f},  N={len(accs)}")
else:
    print("\nNo valid label pairs for accuracy computation.")