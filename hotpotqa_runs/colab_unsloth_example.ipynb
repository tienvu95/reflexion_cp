{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies (run once).\n",
    "# In Colab, prefix with `!` when running these commands in a code cell.\n",
    "!pip install -q unsloth transformers accelerate bitsandbytes safetensors fastapi uvicorn\n",
    "# Restart the kernel after large installs if needed (Colab may require runtime restart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145b1bd",
   "metadata": {},
   "source": [
    "Cell 2: Load repository and the UnsloTh model into a variable named `llm`. Run this cell once; subsequent cells can reuse `llm` without reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load model once\n",
    "import os\n",
    "import sys\n",
    "# Ensure repo root is on sys.path (adjust path if your notebook root differs)\n",
    "sys.path.insert(0, '/content/reflexion')  # change to the path where you cloned the repo\n",
    "from hotpotqa_runs.unsloth_llm import UnslothLLM\n",
    "# Set HF token if you need to access gated models\n",
    "os.environ['HUGGINGFACE_API_TOKEN'] = '<YOUR_HF_TOKEN>'  # or set in notebook secrets\n",
    "MODEL = 'unsloth/Meta-Llama-3.1-8B-bnb-4bit'\n",
    "# Create the LLM once. This may take a few minutes the first time.\n",
    "llm = UnslothLLM(model_name=MODEL, token=os.environ.get('HUGGINGFACE_API_TOKEN'), load_in_4bit=True, max_seq_length=8192)\n",
    "print('LLM ready:', llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92570438",
   "metadata": {},
   "source": [
    "Cell 3: Call the runner with the pre-initialized `llm`. Build an argparse-like namespace and pass `external_llm=llm` to `run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: run the PubMedQA runner using the already-loaded llm\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "from hotpotqa_runs import run_pubmedqa\n",
    "\n",
    "# Construct an args-like object. Adjust dataset/model/limit as needed.\n",
    "args = SimpleNamespace(\n",
    "    dataset='qiaojin/PubMedQA',\n",
    "    split='validation',\n",
    "    limit=5,\n",
    "    model='unsloth/Meta-Llama-3.1-8B-bnb-4bit',\n",
    "    use_transformers=False,\n",
    "    use_unsloth=True,\n",
    "    hf_token=os.environ.get('HUGGINGFACE_API_TOKEN'),\n",
    "    out=None,\n",
    "    agent='react',\n",
    "    dataset_config='pqa_labeled',\n",
    "    reflexion_strategy='reflexion',\n",
    "    max_steps=6,\n",
    "    question_field=None,\n",
    "    context_field=None,\n",
    "    answer_field=None,\n",
    "    device=None,\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=8192,\n",
    ")\n",
    "\n",
    "# Call runner with external llm to avoid reloading the model\n",
    "run_pubmedqa.run(args, external_llm=llm)\n",
    "\n",
    "# You can now re-run this cell multiple times with different `args.limit` or dataset slices without reloading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4bc764",
   "metadata": {},
   "source": [
    "Cell 4: Optional â€” interactive prompt using the same `llm` (quick tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe69ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: quick interactive call\n",
    "prompt = 'Summarize the clinical problem of diabetes in one sentence.'\n",
    "print(llm(prompt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
