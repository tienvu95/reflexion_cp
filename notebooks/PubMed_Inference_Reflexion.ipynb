{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8ce43d",
   "metadata": {},
   "source": [
    "# PubMedQA inference + Reflexion (Colab-ready)\n",
    "\n",
    "This notebook is a Colab-friendly conversion of `pubmed_inference_colab.py` and adds an inline reflexion loop: when the model's first 1-token label is incorrect, we ask the model to reflect briefly and then retry the label generation with the reflection appended to the prompt.\n",
    "\n",
    "This version uses the Unsloth FastLanguageModel (or another Hugging Face-compatible tokenizer/model that supports `generate` and `tokenizer.apply_chat_template`). No OpenAI API is required.\n",
    "\n",
    "Notes:\n",
    "- If you upload the whole repo to Colab, you can also import `hotpotqa_runs` helpers; here the notebook is self-contained and avoids LangChain to reduce dependencies.\n",
    "- Keep `N` small for a smoke test (e.g., 5) before scaling up.\n",
    "markdown\n",
    "markdown\n",
    "# PubMedQA inference + Reflexion (Colab-ready)\n",
    "\n",
    "This notebook reproduces the PubMed inference flow and demonstrates an inline reflexion strategy (reflect-then-retry) using an Unsloth / Hugging Face model on Google Colab. It avoids OpenAI and LangChain so it is quicker to run in Colab.\n",
    "\n",
    "Workflow:\n",
    "- Load model + tokenizer (Unsloth FastLanguageModel or compatible HF tokenizer).\n",
    "- Run single-token label classification for PubMedQA (yes/no/maybe) by generating exactly 1 token.\n",
    "- If the first label is incorrect, ask the model to produce a short reflection and retry the label with the reflection included.\n",
    "\n",
    "Keep `N` small for a smoke test (e.g., 5).\n",
    "code\n",
    "python\n",
    "# Install dependencies (run once in Colab).\n",
    "# Adjust package list if you already have some packages installed.\n",
    "!pip install -q unsloth datasets transformers accelerate textstat evaluate rouge_score scikit-learn tqdm\n",
    "markdown\n",
    "markdown\n",
    "## Load model and tokenizer\n",
    "The notebook uses Unsloth's `FastLanguageModel` which is convenient in Colab. If you prefer a raw Hugging Face model/tokenizer, replace this cell accordingly.\n",
    "code\n",
    "python\n",
    "# Load model and tokenizer (Unsloth)\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Choose model: if the exact model isn't available, replace with another Unsloth or HF model name.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct',\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Apply chat template for consistent chat-style tokenization (same as original script)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = 'llama-3.1',\n",
    "    mapping = { 'role': 'from', 'content': 'value', 'user': 'human', 'assistant': 'gpt' },\n",
    ")\n",
    "\n",
    "# Enable inference mode helper (Unsloth)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print('Model and tokenizer loaded')\n",
    "markdown\n",
    "markdown\n",
    "## Helpers: prompts, parsing, and probability extraction\n",
    "These utilities reproduce the single-token classification behavior in the original script.\n",
    "code\n",
    "python\n",
    "import re, gc, torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import textstat\n",
    "\n",
    "SYSTEM  = 'Answer ONLY with one word: yes, no, or maybe.'\n",
    "CLASSES = ['yes','no','maybe']\n",
    "CLASS_SET = set(CLASSES)\n",
    "MAX_INP = 768\n",
    "\n",
    "def build_messages(q, ctx):\n",
    "    if isinstance(ctx, list): ctx = ' '.join(ctx)\n",
    "    return [{\n",
    "        'from': 'human',\n",
    "        'value': (\n",
    "            f\"{SYSTEM}\\n\\nQuestion: {q}\\n\\nAbstract:\\n{ctx}\\n\\n\n",
    ",\n",
    ",\n",
    "\n",
    ",\n",
    ",\n",
    ",\n",
    "1e-12\n",
    ",\n",
    ",\n",
    ","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
